{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Problem Statement\n","\n","<img src=https://www.matherhospital.org/wp-content/uploads/2017/09/brain-tumor-blog-photo.jpg width=500/>\n","\n","A brain tumor is a mass or growth of abnormal cells in your brain.\n","Many different types of brain tumors exist. Some brain tumors are noncancerous (benign), and some brain tumors are cancerous (malignant). Brain tumors can begin in your brain (primary brain tumors), or cancer can begin in other parts of your body and spread to your brain as secondary (metastatic) brain tumors.\n","How quickly a brain tumor grows can vary greatly. The growth rate as well as the location of a brain tumor determines how it will affect the function of your nervous system.\n","Brain tumor treatment options depend on the type of brain tumor you have, as well as its size and location.\n","\n","Our goal now is to design a deep learning system based on both vision transformers and Convolutional neural networks and compare both of them using different metrics."]},{"cell_type":"markdown","metadata":{},"source":["## Convolutional Neural Network\n","\n","In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of artificial neural network (ANN), most commonly applied to analyze visual imagery. CNNs are also known as Shift Invariant or Space Invariant Artificial Neural Networks (SIANN), based on the shared-weight architecture of the convolution kernels or filters that slide along input features and provide translation-equivariant responses known as feature maps. Counter-intuitively, most convolutional neural networks are not invariant to translation, due to the downsampling operation they apply to the input. They have applications in image and video recognition, recommender systems, image classification, image segmentation, medical image analysis, natural language processing, brainâ€“computer interfaces, and financial time series.\n","\n","<img src= https://cdn.discuss.boardinfinity.com/original/2X/1/1f1bf9539699c880b33f978e724f803ef8197f6f.png width=700/>\n","\n","CNNs are regularized versions of multilayer perceptrons. Multilayer perceptrons usually mean fully connected networks, that is, each neuron in one layer is connected to all neurons in the next layer. The \"full connectivity\" of these networks make them prone to overfitting data. Typical ways of regularization, or preventing overfitting, include: penalizing parameters during training (such as weight decay) or trimming connectivity (skipped connections, dropout, etc.) CNNs take a different approach towards regularization: they take advantage of the hierarchical pattern in data and assemble patterns of increasing complexity using smaller and simpler patterns embossed in their filters. Therefore, on a scale of connectivity and complexity, CNNs are on the lower extreme. "]},{"cell_type":"markdown","metadata":{},"source":["## Vision Transformers(ViT)\n","\n","The concept of Vision Transformer (ViT) is an extension of the original concept of Transformer. It is only the application of Transformer in the image domain with slight modification in the implementation in order to handle the different data modality. More specifically, a ViT uses different methods for tokenization and embedding. However, the generic architecture remains the same. An input image is split into a set of image patches, called visual tokens. The visual tokens are embedded into a set of encoded vectors of fixed dimension. The position of a patch in the image is embedded along with the encoded vector and fed into the transformer encoder network which is essentially the same as the one responsible for processing the text input. \n","\n","<img src=https://miro.medium.com/max/700/1*_c8SqxPMY_dsApyvDJ8HtA.gif width=600/>\n","\n","There are multiple blocks in the ViT encoder and each block consists of three major processing elements: Layer Norm, Multi-head Attention Network (MSP) and Multi-Layer Perceptrons (MLP). Layer Norm keeps the training process on track and let model adapt to the variations among the training images. MSP is a network responsible for generation of attention maps from the given embedded visual tokens. These attention maps help network focus on most important regions in the image such as object(s). "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T18:34:07.530643Z","iopub.status.busy":"2024-07-02T18:34:07.530233Z","iopub.status.idle":"2024-07-02T18:34:08.592053Z","shell.execute_reply":"2024-07-02T18:34:08.590966Z","shell.execute_reply.started":"2024-07-02T18:34:07.530557Z"},"trusted":true},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"markdown","metadata":{},"source":["## Importing necessary libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-07-02T18:34:13.996195Z","iopub.status.busy":"2024-07-02T18:34:13.995643Z","iopub.status.idle":"2024-07-02T18:34:34.854875Z","shell.execute_reply":"2024-07-02T18:34:34.853799Z","shell.execute_reply.started":"2024-07-02T18:34:13.996152Z"},"trusted":true},"outputs":[],"source":["!pip install visualkeras\n","import os\n","import warnings\n","import itertools\n","import cv2\n","import seaborn as sns\n","import pandas as pd\n","import numpy  as np\n","from PIL import Image\n","from sklearn.utils import class_weight\n","from sklearn.metrics import confusion_matrix, classification_report\n","from collections import Counter\n","\n","import tensorflow as tf\n","import tensorflow_addons as tfa\n","import visualkeras\n","import plotly.express as px\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import multilabel_confusion_matrix\n","\n","from tensorflow.keras.preprocessing.image import load_img\n","from tensorflow.keras.utils import plot_model\n","from tensorflow.keras import layers\n","from tensorflow.keras import regularizers\n","from sklearn.model_selection   import train_test_split\n","from keras.preprocessing.image import ImageDataGenerator\n","\n","warnings.filterwarnings('ignore')\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{},"source":["## Setting up general parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T18:34:34.857502Z","iopub.status.busy":"2024-07-02T18:34:34.857092Z","iopub.status.idle":"2024-07-02T18:34:34.863564Z","shell.execute_reply":"2024-07-02T18:34:34.862235Z","shell.execute_reply.started":"2024-07-02T18:34:34.857467Z"},"trusted":true},"outputs":[],"source":["# General parameters\n","epochs = 15\n","pic_size = 240\n","np.random.seed(42)\n","tf.random.set_seed(42)"]},{"cell_type":"markdown","metadata":{},"source":["## Data Loading, Preperation and Visualization"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T18:34:34.865660Z","iopub.status.busy":"2024-07-02T18:34:34.865265Z","iopub.status.idle":"2024-07-02T18:34:38.880311Z","shell.execute_reply":"2024-07-02T18:34:38.879445Z","shell.execute_reply.started":"2024-07-02T18:34:34.865625Z"},"trusted":true},"outputs":[],"source":["folder_path = \"C:/Users/Shiven/Downloads/Brain tumor Detection/Brain_Tumor_Dataset\"\n","no_images = os.listdir(folder_path + '/no/')\n","yes_images = os.listdir(folder_path + '/yes/')\n","dataset=[]\n","lab=[]\n","\n","for image_name in no_images:\n","    image=cv2.imread(folder_path + '/no/' + image_name)\n","    image=Image.fromarray(image,'RGB')\n","    image=image.resize((240,240))\n","    dataset.append(np.array(image))\n","    lab.append(0)\n","    \n","for image_name in yes_images:\n","    image=cv2.imread(folder_path + '/yes/' + image_name)\n","    image=Image.fromarray(image,'RGB')\n","    image=image.resize((240,240))\n","    dataset.append(np.array(image))\n","    lab.append(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T18:34:38.883288Z","iopub.status.busy":"2024-07-02T18:34:38.882945Z","iopub.status.idle":"2024-07-02T18:34:38.902819Z","shell.execute_reply":"2024-07-02T18:34:38.901887Z","shell.execute_reply.started":"2024-07-02T18:34:38.883258Z"},"trusted":true},"outputs":[],"source":["dataset = np.array(dataset)\n","lab = np.array(lab)\n","print(dataset.shape, lab.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T18:34:38.904221Z","iopub.status.busy":"2024-07-02T18:34:38.903901Z","iopub.status.idle":"2024-07-02T18:34:38.921446Z","shell.execute_reply":"2024-07-02T18:34:38.920505Z","shell.execute_reply.started":"2024-07-02T18:34:38.904192Z"},"trusted":true},"outputs":[],"source":["x_train, x_test, y_train, y_test = train_test_split(dataset, lab, test_size=0.2, shuffle=True, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T18:34:38.922875Z","iopub.status.busy":"2024-07-02T18:34:38.922569Z","iopub.status.idle":"2024-07-02T18:34:38.929336Z","shell.execute_reply":"2024-07-02T18:34:38.928374Z","shell.execute_reply.started":"2024-07-02T18:34:38.922847Z"},"trusted":true},"outputs":[],"source":["def plot_state(state):\n","    plt.figure(figsize= (12,12))\n","    for i in range(1, 10, 1):\n","        plt.subplot(3,3,i)\n","        img = load_img(folder_path + \"/\" + state + \"/\" + os.listdir(folder_path + \"/\" + state)[i], target_size=(pic_size, pic_size))\n","        plt.imshow(img)   \n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T18:34:38.930924Z","iopub.status.busy":"2024-07-02T18:34:38.930590Z","iopub.status.idle":"2024-07-02T18:34:40.294418Z","shell.execute_reply":"2024-07-02T18:34:40.293437Z","shell.execute_reply.started":"2024-07-02T18:34:38.930887Z"},"trusted":true},"outputs":[],"source":["plot_state('yes')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T18:34:40.295881Z","iopub.status.busy":"2024-07-02T18:34:40.295575Z","iopub.status.idle":"2024-07-02T18:34:41.349794Z","shell.execute_reply":"2024-07-02T18:34:41.348822Z","shell.execute_reply.started":"2024-07-02T18:34:40.295851Z"},"trusted":true},"outputs":[],"source":["plot_state(\"no\")"]},{"cell_type":"markdown","metadata":{},"source":["## Modeling using Vision Transformers(ViT)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T18:34:41.351446Z","iopub.status.busy":"2024-07-02T18:34:41.351100Z","iopub.status.idle":"2024-07-02T18:34:41.358621Z","shell.execute_reply":"2024-07-02T18:34:41.357206Z","shell.execute_reply.started":"2024-07-02T18:34:41.351414Z"},"trusted":true},"outputs":[],"source":["learning_rate = 0.001\n","weight_decay = 0.0001\n","batch_size = 256\n","num_epochs = 100\n","image_size = 240  # We'll resize input images to this size\n","patch_size = 20  # Size of the patches to be extract from the input images\n","num_patches = (image_size // patch_size) ** 2\n","projection_dim = 64\n","num_heads = 4\n","transformer_units = [\n","    projection_dim * 2,\n","    projection_dim,\n","]  # Size of the transformer layers\n","transformer_layers = 8\n","mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier"]},{"cell_type":"markdown","metadata":{},"source":["## Data augmentation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T18:34:41.362738Z","iopub.status.busy":"2024-07-02T18:34:41.362407Z","iopub.status.idle":"2024-07-02T18:34:45.142466Z","shell.execute_reply":"2024-07-02T18:34:45.141594Z","shell.execute_reply.started":"2024-07-02T18:34:41.362696Z"},"trusted":true},"outputs":[],"source":["data_augmentation = tf.keras.Sequential(\n","    [\n","        layers.Normalization(),\n","        layers.Resizing(image_size, image_size),\n","        layers.RandomFlip(\"horizontal\"),\n","        layers.RandomRotation(factor=0.02),\n","        layers.RandomZoom(\n","            height_factor=0.2, width_factor=0.2\n","        ),\n","    ],\n","    name=\"data_augmentation\",\n",")\n","# Compute the mean and the variance of the training data for normalization.\n","data_augmentation.layers[0].adapt(x_train)"]},{"cell_type":"markdown","metadata":{},"source":["## Multi-layer perceptron"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T18:34:45.144602Z","iopub.status.busy":"2024-07-02T18:34:45.143939Z","iopub.status.idle":"2024-07-02T18:34:45.150377Z","shell.execute_reply":"2024-07-02T18:34:45.149358Z","shell.execute_reply.started":"2024-07-02T18:34:45.144559Z"},"trusted":true},"outputs":[],"source":["def mlp(x, hidden_units, dropout_rate):\n","    for units in hidden_units:\n","        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n","        x = layers.Dropout(dropout_rate)(x)\n","    return x"]},{"cell_type":"markdown","metadata":{},"source":["## Implement patch creation as a layer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T18:34:45.151891Z","iopub.status.busy":"2024-07-02T18:34:45.151598Z","iopub.status.idle":"2024-07-02T18:34:45.160938Z","shell.execute_reply":"2024-07-02T18:34:45.160005Z","shell.execute_reply.started":"2024-07-02T18:34:45.151864Z"},"trusted":true},"outputs":[],"source":["class Patches(layers.Layer):\n","    def __init__(self, patch_size):\n","        super(Patches, self).__init__()\n","        self.patch_size = patch_size\n","\n","    def call(self, images):\n","        batch_size = tf.shape(images)[0]\n","        patches = tf.image.extract_patches(\n","            images=images,\n","            sizes=[1, self.patch_size, self.patch_size, 1],\n","            strides=[1, self.patch_size, self.patch_size, 1],\n","            rates=[1, 1, 1, 1],\n","            padding=\"VALID\",\n","        )\n","        patch_dims = patches.shape[-1]\n","        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n","        return patches"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T18:34:45.162306Z","iopub.status.busy":"2024-07-02T18:34:45.161984Z","iopub.status.idle":"2024-07-02T18:34:50.646884Z","shell.execute_reply":"2024-07-02T18:34:50.645754Z","shell.execute_reply.started":"2024-07-02T18:34:45.162280Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(8, 8))\n","image = x_train[np.random.choice(range(x_train.shape[0]))]\n","plt.imshow(image.astype(\"uint8\"))\n","\n","resized_image = tf.image.resize(\n","    tf.convert_to_tensor([image]), size=(image_size, image_size)\n",")\n","patches = Patches(patch_size)(resized_image)\n","print(f\"Image size: {image_size} X {image_size}\")\n","print(f\"Patch size: {patch_size} X {patch_size}\")\n","print(f\"Patches per image: {patches.shape[1]}\")\n","print(f\"Elements per patch: {patches.shape[-1]}\")\n","\n","n = int(np.sqrt(patches.shape[1]))\n","plt.figure(figsize=(8, 8))\n","for i, patch in enumerate(patches[0]):\n","    ax = plt.subplot(n, n, i + 1)\n","    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n","    plt.imshow(patch_img.numpy().astype(\"uint8\"))\n","    plt.axis(\"off\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## Creating the patch encoder\n","The PatchEncoder layer will linearly transform a patch by projecting it into a vector of size projection_dim. In addition, it adds a learnable position embedding to the projected vector."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T18:34:50.648799Z","iopub.status.busy":"2024-07-02T18:34:50.648359Z","iopub.status.idle":"2024-07-02T18:34:50.657689Z","shell.execute_reply":"2024-07-02T18:34:50.656432Z","shell.execute_reply.started":"2024-07-02T18:34:50.648752Z"},"trusted":true},"outputs":[],"source":["class PatchEncoder(tf.keras.layers.Layer):\n","    def __init__(self, num_patches, projection_dim):\n","        super(PatchEncoder, self).__init__()\n","        self.num_patches = num_patches\n","        self.projection = layers.Dense(units=projection_dim)\n","        self.position_embedding = layers.Embedding(\n","            input_dim=num_patches, output_dim=projection_dim\n","        )\n","\n","    def call(self, patch):\n","        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n","        encoded = self.projection(patch) + self.position_embedding(positions)\n","        return encoded"]},{"cell_type":"markdown","metadata":{},"source":["## Building the ViT\n","\n","The ViT model consists of multiple Transformer blocks, which use the layers.MultiHeadAttention layer as a self-attention mechanism applied to the sequence of patches. The Transformer blocks produce a [batch_size, num_patches, projection_dim] tensor, which is processed via an classifier head with softmax to produce the final class probabilities output.\n","\n","Unlike the technique described in the paper, which prepends a learnable embedding to the sequence of encoded patches to serve as the image representation, all the outputs of the final Transformer block are reshaped with layers.Flatten() and used as the image representation input to the classifier head. Note that the layers.GlobalAveragePooling1D layer could also be used instead to aggregate the outputs of the Transformer block, especially when the number of patches and the projection dimensions are large."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T18:34:50.659659Z","iopub.status.busy":"2024-07-02T18:34:50.659248Z","iopub.status.idle":"2024-07-02T18:34:50.672344Z","shell.execute_reply":"2024-07-02T18:34:50.670943Z","shell.execute_reply.started":"2024-07-02T18:34:50.659615Z"},"trusted":true},"outputs":[],"source":["def create_vit_classifier():\n","    inputs = layers.Input(shape=(240, 240, 3))\n","    # Augment data.\n","    augmented = data_augmentation(inputs)\n","    # Create patches.\n","    patches = Patches(patch_size)(augmented)\n","    # Encode patches.\n","    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n","\n","    # Create multiple layers of the Transformer block.\n","    for _ in range(transformer_layers):\n","        # Layer normalization 1.\n","        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n","        # Create a multi-head attention layer.\n","        attention_output = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n","        )(x1, x1)\n","        # Skip connection 1.\n","        x2 = layers.Add()([attention_output, encoded_patches])\n","        # Layer normalization 2.\n","        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n","        # MLP.\n","        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n","        # Skip connection 2.\n","        encoded_patches = layers.Add()([x3, x2])\n","\n","    # Create a [batch_size, projection_dim] tensor.\n","    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n","    representation = layers.Flatten()(representation)\n","    representation = layers.Dropout(0.5)(representation)\n","    # Add MLP.\n","    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n","    # Classify outputs.\n","    logits = layers.Dense(2)(features)\n","    # Create the Keras model.\n","    model = tf.keras.Model(inputs=inputs, outputs=logits)\n","    return model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T18:34:50.674266Z","iopub.status.busy":"2024-07-02T18:34:50.673820Z","iopub.status.idle":"2024-07-02T18:34:50.685223Z","shell.execute_reply":"2024-07-02T18:34:50.684221Z","shell.execute_reply.started":"2024-07-02T18:34:50.674216Z"},"trusted":true},"outputs":[],"source":["def run_experiment(model):\n","    optimizer = tfa.optimizers.AdamW(\n","        learning_rate=learning_rate, weight_decay=weight_decay\n","    )\n","\n","    model.compile(\n","        optimizer=optimizer,\n","        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","        metrics=[\n","            tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n","            tf.keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n","        ],\n","    )\n","\n","    checkpoint_filepath = \"/tmp/checkpoint\"\n","    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","        checkpoint_filepath,\n","        monitor=\"val_accuracy\",\n","        save_best_only=True,\n","        save_weights_only=True,\n","    )\n","\n","    history = model.fit(\n","        x=x_train,\n","        y=y_train,\n","        batch_size=batch_size,\n","        epochs=num_epochs,\n","        validation_data=(x_test, y_test),\n","        callbacks=[checkpoint_callback],\n","    )\n","\n","    model.load_weights(checkpoint_filepath)\n","    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n","    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n","    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n","\n","    return history"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","checkpoint_dir = \"./checkpoint\"\n","if not os.path.exists(checkpoint_dir):\n","    os.makedirs(checkpoint_dir)\n","checkpoint_filepath = os.path.join(checkpoint_dir, \"model_checkpoint\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T18:34:50.686926Z","iopub.status.busy":"2024-07-02T18:34:50.686591Z","iopub.status.idle":"2024-07-02T18:35:51.625854Z","shell.execute_reply":"2024-07-02T18:35:51.624787Z","shell.execute_reply.started":"2024-07-02T18:34:50.686895Z"},"trusted":true},"outputs":[],"source":["vit_classifier = create_vit_classifier()\n","vit_history = run_experiment(vit_classifier)"]},{"cell_type":"markdown","metadata":{},"source":["## Notes about the ViT performance\n","\n","The state of the art results reported in the paper are achieved by pre-training the ViT model using the JFT-300M dataset, then fine-tuning it on the target dataset. To improve the model quality without pre-training, you can try to train the model for more epochs, use a larger number of Transformer layers, resize the input images, change the patch size, or increase the projection dimensions. Besides, as mentioned in the paper, the quality of the model is affected not only by architecture choices, but also by parameters such as the learning rate schedule, optimizer, weight decay, etc. In practice, it's recommended to fine-tune a ViT model that was pre-trained using a large, high-resolution dataset."]},{"cell_type":"markdown","metadata":{},"source":["## ViT model Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T18:35:51.630405Z","iopub.status.busy":"2024-07-02T18:35:51.629765Z","iopub.status.idle":"2024-07-02T18:35:51.979259Z","shell.execute_reply":"2024-07-02T18:35:51.978267Z","shell.execute_reply.started":"2024-07-02T18:35:51.630371Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(20,10))\n","plt.subplot(1, 2, 1)\n","plt.suptitle('Optimizer : Adam', fontsize=10)\n","plt.ylabel('Loss', fontsize=16)\n","plt.plot(vit_history.history['loss'], label='Training Loss')\n","plt.plot(vit_history.history['val_loss'], label='Validation Loss')\n","plt.legend(loc='upper right')\n","\n","plt.subplot(1, 2, 2)\n","plt.ylabel('Accuracy', fontsize=16)\n","plt.plot(vit_history.history['accuracy'], label='Training Accuracy')\n","plt.plot(vit_history.history['val_accuracy'], label='Validation Accuracy')\n","plt.legend(loc='lower right')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T18:35:51.980808Z","iopub.status.busy":"2024-07-02T18:35:51.980484Z","iopub.status.idle":"2024-07-02T18:35:53.120331Z","shell.execute_reply":"2024-07-02T18:35:53.119113Z","shell.execute_reply.started":"2024-07-02T18:35:51.980773Z"},"trusted":true},"outputs":[],"source":["# compute predictions\n","vit_predictions = vit_classifier.predict(x_test)\n","vit_y_pred = [np.argmax(probas) for probas in vit_predictions]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T18:36:46.003818Z","iopub.status.busy":"2024-07-02T18:36:46.003366Z","iopub.status.idle":"2024-07-02T18:36:46.043763Z","shell.execute_reply":"2024-07-02T18:36:46.042246Z","shell.execute_reply.started":"2024-07-02T18:36:46.003781Z"},"trusted":true},"outputs":[],"source":["# compute confusion matrix\n","cnf_matrix = confusion_matrix(y_test, vit_y_pred)\n","np.set_printoptions(precision=2)\n","# plot normalized confusion matrix\n","plt.figure()\n","plot_confusion_matrix(cnf_matrix, classes=[\"Yes\", \"No\"], title='Normalized confusion matrix')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## General CNN vs. ViT talk\n","\n","<img src = https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F918d92aa-65a5-43c0-b58c-897007a8bfa5_2908x929.png width = 700/>\n","\n","The differences between CNNs and Vision Transformers are many and lie mainly in their architectural differences.\n","In fact, CNNs achieve excellent results even with training based on data volumes that are not as large as those required by Vision Transformers.\n","This different behaviour seems to derive from the presence in the CNNs of some inductive biases that can be somehow exploited by these networks to grasp more quickly the particularities of the analysed images even if, on the other hand, they end up limiting them making it more complex to grasp global relations.\n","\n","On the other hand, the Vision Transformers are free from these biases which leads them to be able to capture also global and wider range relations but at the cost of a more onerous training in terms of data.\n","Vision Transformers also proved to be much more robust to input image distortions such as adversarial patches or permutations.\n","However, choosing one architecture over another is not always the wisest choice, and excellent results have been obtained in several Computer Vision tasks through hybrid architectures combining convolutional layers with Vision Transformers."]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":165566,"sourceId":377107,"sourceType":"datasetVersion"}],"dockerImageVersionId":30262,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"}},"nbformat":4,"nbformat_minor":4}
